{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "HTRU2 is a data set which describes a sample of pulsar candidates collected during the\n",
    "High Time Resolution Universe Survey (South) [1]. Pulsars are a rare type of Neutron star that produce radio emission detectable here on Earth. They are of considerable scientific interest as probes of space-time, the interstellar medium, and states of matter (see [2] for more uses). As pulsars rotate, their emission beam sweeps across the sky, and when this crosses our line of sight, produces a detectable pattern of broadband radio emission. As pulsars rotate rapidly, this pattern repeats periodically. Thus pulsar search involves looking for periodic radio signals with large radio telescopes. Each pulsar produces a slightly different emission pattern, which varies slightly with each rotation (see [2] for an introduction to pulsar astrophysics to find out why). Thus a potential signal detection known as a 'candidate', is averaged over many rotations of the pulsar, as determined by the length of an observation. In the absence of additional info, each candidate could potentially describe a real pulsar. However in practice almost all detections are caused by radio frequency interference (RFI) and noise, making legitimate signals hard to find. \n",
    "Machine learning tools are now being used to automatically label pulsar candidates to facilitate rapid analysis. Classification systems in particular are being widely adopted,\t(see [4,5,6,7,8,9]) which treat the candidate data sets  as binary classification problems.\tHere the legitimate pulsar examples are a minority positive class, and spurious examples the majority negative class. At present multi-class labels are unavailable, given the costs associated with data annotation.\n",
    "\t\n",
    "The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators. Each candidate is described by 8 continuous variables. The first four are simple statistics obtained from the integrated pulse profile (folded profile). This is an array of continuous variables that describe a longitude-resolved version of the signal that has been averaged in both time and frequency (see [3] for more details). The remaining four variables are\tsimilarly obtained from the DM-SNR curve (again see [3] for more details). These are summarised below:\n",
    "\t\n",
    "\t1. Mean of the integrated profile.\n",
    "\t2. Standard deviation of the integrated profile.\n",
    "\t3. Excess kurtosis of the integrated profile.\n",
    "\t4. Skewness of the integrated profile.\n",
    "\t5. Mean of the DM-SNR curve.\n",
    "\t6. Standard deviation of the DM-SNR curve.\n",
    "\t7. Excess kurtosis of the DM-SNR curve.\n",
    "\t8. Skewness of the DM-SNR curve.\n",
    "\t\n",
    "### Data HTRU 2 Summary\n",
    "\t\n",
    "* 17,898 total examples.\n",
    "* 1,639 positive examples.\n",
    "* 16,259 negative examples.\n",
    "\t\n",
    "\t\n",
    "The data is presented in two formats: CSV and ARFF (used by the WEKA data mining tool).Candidates are stored in both files in separate rows. Each row lists the variables first, and the class label is the final entry. The class labels used are 0 (negative) and 1 (positive).\n",
    "Please not that the data contains no positional information or other astronomical details. It is simply feature data extracted from candidate files using the PulsarFeatureLab tool (see [10]).\n",
    "\n",
    "\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploracion de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import requests as rq\n",
    "import zipfile\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rc(\"font\", size=14)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just run one time\n",
    "os.mkdir('data')\n",
    "#get current directory\n",
    "pwd = os.getcwd()\n",
    "#Download data\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00372/HTRU2.zip'\n",
    "path = os.path.join('data','HTRU_2.zip')\n",
    "response = rq.get(url)\n",
    "with open(path, 'wb') as f:\n",
    "    f.write(response.content)\n",
    "#Unzip data\n",
    "with zipfile.ZipFile(path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(os.path.join(pwd,'data'))\n",
    "#delete files unnecesary\n",
    "os.remove(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and load data\n",
    "path = os.path.join('data','HTRU_2.csv')\n",
    "dataset = pd.read_csv(path, delimiter=',', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign name columns to dataframe\n",
    "features = ['Mean','Std','Excess kurtosis','Skewness','Mean DM-SNR curve','Std DM-SNR curve','Exc kurtosis DM-SNR curve',\n",
    "            'Skewness DM-SNR curve','Class']\n",
    "dataset.columns = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "      <th>Excess kurtosis</th>\n",
       "      <th>Skewness</th>\n",
       "      <th>Mean DM-SNR curve</th>\n",
       "      <th>Std DM-SNR curve</th>\n",
       "      <th>Exc kurtosis DM-SNR curve</th>\n",
       "      <th>Skewness DM-SNR curve</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140.562500</td>\n",
       "      <td>55.683782</td>\n",
       "      <td>-0.234571</td>\n",
       "      <td>-0.699648</td>\n",
       "      <td>3.199833</td>\n",
       "      <td>19.110426</td>\n",
       "      <td>7.975532</td>\n",
       "      <td>74.242225</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102.507812</td>\n",
       "      <td>58.882430</td>\n",
       "      <td>0.465318</td>\n",
       "      <td>-0.515088</td>\n",
       "      <td>1.677258</td>\n",
       "      <td>14.860146</td>\n",
       "      <td>10.576487</td>\n",
       "      <td>127.393580</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103.015625</td>\n",
       "      <td>39.341649</td>\n",
       "      <td>0.323328</td>\n",
       "      <td>1.051164</td>\n",
       "      <td>3.121237</td>\n",
       "      <td>21.744669</td>\n",
       "      <td>7.735822</td>\n",
       "      <td>63.171909</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>136.750000</td>\n",
       "      <td>57.178449</td>\n",
       "      <td>-0.068415</td>\n",
       "      <td>-0.636238</td>\n",
       "      <td>3.642977</td>\n",
       "      <td>20.959280</td>\n",
       "      <td>6.896499</td>\n",
       "      <td>53.593661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88.726562</td>\n",
       "      <td>40.672225</td>\n",
       "      <td>0.600866</td>\n",
       "      <td>1.123492</td>\n",
       "      <td>1.178930</td>\n",
       "      <td>11.468720</td>\n",
       "      <td>14.269573</td>\n",
       "      <td>252.567306</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Mean        Std  Excess kurtosis  Skewness  Mean DM-SNR curve  \\\n",
       "0  140.562500  55.683782        -0.234571 -0.699648           3.199833   \n",
       "1  102.507812  58.882430         0.465318 -0.515088           1.677258   \n",
       "2  103.015625  39.341649         0.323328  1.051164           3.121237   \n",
       "3  136.750000  57.178449        -0.068415 -0.636238           3.642977   \n",
       "4   88.726562  40.672225         0.600866  1.123492           1.178930   \n",
       "\n",
       "   Std DM-SNR curve  Exc kurtosis DM-SNR curve  Skewness DM-SNR curve  Class  \n",
       "0         19.110426                   7.975532              74.242225      0  \n",
       "1         14.860146                  10.576487             127.393580      0  \n",
       "2         21.744669                   7.735822              63.171909      0  \n",
       "3         20.959280                   6.896499              53.593661      0  \n",
       "4         11.468720                  14.269573             252.567306      0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of examples is: 17898\n"
     ]
    }
   ],
   "source": [
    "print('the number of examples is:',dataset.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#how much the dataset is balanced\n",
    "data_label = (dataset['Class'].value_counts(normalize=True, sort=False)*100).plot.bar()\n",
    "data_label.set(ylabel=\"Percent\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.loc[:, dataset.columns != 'Class']\n",
    "y = dataset.loc[:, dataset.columns == 'Class']\n",
    "#randomrize the data\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "#get the test set 10000 examples = 55.9 % of total dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5587, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the size of evaluate set is of : 10000\n",
      "the size of train set is of : 7898\n"
     ]
    }
   ],
   "source": [
    "print('the size of evaluate set is of :',y_test.shape[0])\n",
    "print('the size of train set is of :',y_train.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-sampling using SMOTE [11]\n",
    "\n",
    "I over-sampled only on the training data, because by oversampling only on the training data, none of the information in the test data is being used to create synthetic observations, therefore, no information will bleed from test data into the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of oversampled data is  14352\n",
      "Number of no subscription in oversampled data. Label 0 7176\n",
      "Number of subscription. La 7176\n",
      "Proportion of no subscription data in oversampled data is  0.5\n",
      "Proportion of subscription data in oversampled data is  0.5\n"
     ]
    }
   ],
   "source": [
    "os = SMOTE(random_state=0)\n",
    "os_data_X,os_data_y=os.fit_sample(X_train, y_train.values.ravel())\n",
    "\n",
    "os_data_X = pd.DataFrame(data=os_data_X, columns=features[:8] )\n",
    "os_data_y= pd.DataFrame(data=os_data_y, columns=['Class'])\n",
    "# we can Check the numbers of our data\n",
    "print(\"length of oversampled data is \",len(os_data_X))\n",
    "print(\"Number of no subscription in oversampled data. Label 0\",len(os_data_y[os_data_y['Class']==0]))\n",
    "print(\"Number of subscription. La\",len(os_data_y[os_data_y['Class']==1]))\n",
    "print(\"Proportion of no subscription data in oversampled data is \",len(os_data_y[os_data_y['Class']==0])/len(os_data_X))\n",
    "print(\"Proportion of subscription data in oversampled data is \",len(os_data_y[os_data_y['Class']==1])/len(os_data_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomrize the data again. Por si acaso jajaja\n",
    "os_data_X, os_data_y = shuffle(os_data_X, os_data_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First case. the size of train set is of : 100\n",
      "Second case. the size of train set is of : 500\n",
      "Third case. the size of train set is of : 1000\n",
      "Fourth case. the size of train set is of : 5000\n"
     ]
    }
   ],
   "source": [
    "#case 1 training dataset 100 examples\n",
    "X_train100, Y_train100 = shuffle(os_data_X,os_data_y, n_samples=100, random_state=0)\n",
    "print('First case. the size of train set is of :',X_train100.shape[0])\n",
    "\n",
    "#case 2 training dataset 100 examples\n",
    "X_train500, Y_train500 = shuffle(os_data_X,os_data_y, n_samples=500, random_state=1)\n",
    "print('Second case. the size of train set is of :',X_train500.shape[0])\n",
    "\n",
    "#case 2 training dataset 100 examples\n",
    "X_train1000, Y_train1000 = shuffle(os_data_X,os_data_y, n_samples=1000, random_state=2)\n",
    "print('Third case. the size of train set is of :',X_train1000.shape[0])\n",
    "\n",
    "#case 2 training dataset 100 examples\n",
    "X_train5000, Y_train5000 = shuffle(os_data_X,os_data_y, n_samples=5000, random_state=3)\n",
    "print('Fourth case. the size of train set is of :',X_train5000.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Fitting. First case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='liblinear', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg1 = LogisticRegression(solver='liblinear')\n",
    "logreg1.fit(X_train100, Y_train100.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.95\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97      9083\n",
      "           1       0.70      0.86      0.77       917\n",
      "\n",
      "    accuracy                           0.95     10000\n",
      "   macro avg       0.84      0.91      0.87     10000\n",
      "weighted avg       0.96      0.95      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred1 = logreg1.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred1)))\n",
    "print(classification_report(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Fitting. Second case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.94\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.97      9083\n",
      "           1       0.62      0.92      0.74       917\n",
      "\n",
      "    accuracy                           0.94     10000\n",
      "   macro avg       0.81      0.93      0.85     10000\n",
      "weighted avg       0.96      0.94      0.95     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg2 = LogisticRegression(solver='liblinear')\n",
    "logreg2.fit(X_train500, Y_train500.values.ravel())\n",
    "y_pred2 = logreg2.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred2)))\n",
    "print(classification_report(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Fitting. Third case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.96\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.98      9083\n",
      "           1       0.71      0.93      0.80       917\n",
      "\n",
      "    accuracy                           0.96     10000\n",
      "   macro avg       0.85      0.95      0.89     10000\n",
      "weighted avg       0.97      0.96      0.96     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg3 = LogisticRegression(solver='liblinear')\n",
    "logreg3.fit(X_train1000, Y_train1000.values.ravel())\n",
    "y_pred3 = logreg3.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred3)))\n",
    "print(classification_report(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model Fitting. Fourth case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier on test set: 0.97\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98      9083\n",
      "           1       0.75      0.93      0.83       917\n",
      "\n",
      "    accuracy                           0.97     10000\n",
      "   macro avg       0.87      0.95      0.91     10000\n",
      "weighted avg       0.97      0.97      0.97     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logreg4 = LogisticRegression(solver='liblinear')\n",
    "logreg4.fit(X_train5000, Y_train5000.values.ravel())\n",
    "y_pred4 = logreg4.predict(X_test)\n",
    "print('Accuracy of logistic regression classifier on test set: {:.2f}'.format(accuracy_score(y_test, y_pred4)))\n",
    "print(classification_report(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "The above result shows how to increase the data available to training set improve the accuracy over test set using the same parameters and setting of the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\t[1] M.~J. Keith et al., \"The High Time Resolution Universe Pulsar Survey - I. System Configuration \n",
    "\t    and Initial Discoveries\",2010, Monthly Notices of the Royal Astronomical Society, vol. 409,\n",
    "\t    pp. 619-627. DOI: 10.1111/j.1365-2966.2010.17325.x\n",
    "\t\n",
    "\t[2] D. R. Lorimer and M. Kramer, \"Handbook of Pulsar Astronomy\", Cambridge University Press, 2005.\n",
    "\t\n",
    "\t[3] R. J. Lyon, \"Why Are Pulsars Hard To Find?\", PhD Thesis, University of Manchester, 2015.\n",
    "\t\n",
    "\t[4] R. J. Lyon et al., \"Fifty Years of Pulsar Candidate Selection: From simple filters to a new\n",
    "\t\tprincipled real-time classification approach\", Monthly Notices of the Royal Astronomical Society,\n",
    "\t\tsubmitted.\n",
    "\t\t\n",
    "\t[5] R. P. Eatough et al., \"Selection of radio pulsar candidates using artificial neural networks\",\n",
    "\t\tMonthly Notices of the Royal Astronomical Society, vol. 407, no. 4, pp. 2443-2450, 2010.\n",
    "\t\t\n",
    "\t[6] S. D. Bates et al., \"The high time resolution universe pulsar survey vi. an artificial neural\n",
    "\t\tnetwork and timing of 75 pulsars\", Monthly Notices of the Royal Astronomical Society, vol. 427,\n",
    "\t\tno. 2, pp. 1052-1065, 2012.\n",
    "\n",
    "\t[7] D. Thornton, \"The High Time Resolution Radio Sky\", PhD thesis, University of Manchester,\n",
    "\t\tJodrell Bank Centre for Astrophysics School of Physics and Astronomy, 2013.\n",
    "\t\t\n",
    "\t[8] K. J. Lee et al., \"PEACE: pulsar evaluation algorithm for candidate extraction a software package\n",
    "\t\tfor post-analysis processing of pulsar survey candidates\", Monthly Notices of the Royal Astronomical\n",
    "\t\tSociety, vol. 433, no. 1, pp. 688-694, 2013.\n",
    "\t\t\n",
    "\t[9] V. Morello et al., \"SPINN: a straightforward machine learning solution to the pulsar candidate\n",
    "\t\tselection problem\", Monthly Notices of the Royal Astronomical Society, vol. 443, no. 2,\n",
    "\t\tpp. 1651-1662, 2014.\n",
    "\t\t\n",
    "\t[10] R. J. Lyon, \"PulsarFeatureLab\", 2015, https://dx.doi.org/10.6084/m9.figshare.1536472.v1.\n",
    "    \n",
    "    [11] Chawla, N. V., et al. “SMOTE: Synthetic Minority Over-Sampling Technique.” Journal of Artificial Intelligence Research, vol. 16, Jan. 2002, pp. 321–357., doi:10.1613/jair.953."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
